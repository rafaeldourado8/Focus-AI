# FOCUS AI
## AnÃ¡lise de Funcionalidades e Melhorias

**Documento de EspecificaÃ§Ã£o TÃ©cnica**  
*Janeiro 2026*

---

## 1. Resumo Executivo

O Focus AI Ã© uma aplicaÃ§Ã£o de chat LLM voltada para programadores, atualmente em desenvolvimento. Esta anÃ¡lise identifica funcionalidades ausentes e propÃµe melhorias para tornar a ferramenta mais competitiva e Ãºtil para desenvolvedores.

> âš ï¸ **Problema identificado:** O container Ollama estÃ¡ falhando ao iniciar, impedindo o funcionamento completo da aplicaÃ§Ã£o. AlÃ©m disso, funcionalidades essenciais para um chat LLM profissional estÃ£o ausentes.

---

## 2. Estado Atual da AplicaÃ§Ã£o

### âœ“ Funcionalidades Existentes

- Interface de chat bÃ¡sica com design dark mode
- IntegraÃ§Ã£o com backend FastAPI
- Container Docker para Ollama (com erro)
- Container PostgreSQL para persistÃªncia
- Container Redis para cache
- Frontend em localhost:5173 (Vite/React)
- ExibiÃ§Ã£o de blocos de cÃ³digo com formataÃ§Ã£o bÃ¡sica

### âœ— Funcionalidades Ausentes CrÃ­ticas

- Sidebar de histÃ³rico de conversas
- Indicador de janela de contexto
- VisualizaÃ§Ã£o de tokenizaÃ§Ã£o
- Sistema de projetos/workspaces
- ConfiguraÃ§Ãµes de modelo

---

## 3. Melhorias Detalhadas

### ğŸ“ 3.1 Sidebar de HistÃ³rico de Conversas

A sidebar Ã© essencial para navegaÃ§Ã£o entre conversas e organizaÃ§Ã£o do trabalho do desenvolvedor.

#### Requisitos Funcionais

- Sidebar retrÃ¡til (toggle com Ã­cone ou atalho `Ctrl+B`)
- Lista de conversas com tÃ­tulo, preview e timestamp
- Campo de busca com filtro em tempo real
- Agrupamento por: Hoje, Ontem, Ãšltimos 7 dias, Mais antigos
- OpÃ§Ã£o de criar pastas/projetos para organizaÃ§Ã£o
- Drag and drop para reorganizar conversas
- Menu de contexto: Renomear, Excluir, Mover, Exportar
- Indicador visual de conversa ativa

#### EspecificaÃ§Ã£o de UI

| Elemento | EspecificaÃ§Ã£o |
|----------|---------------|
| Largura | 280px expandida, 60px colapsada (apenas Ã­cones) |
| AnimaÃ§Ã£o | TransiÃ§Ã£o suave de 200ms ease-in-out |
| PersistÃªncia | Estado salvo em localStorage |
| Responsivo | Overlay em telas < 768px |

#### Wireframe Sugerido

```
â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  â‰¡  â”‚  Focus AI                            âš™ï¸   â”‚
â”œâ”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚     â”‚                                            â”‚
â”‚ ğŸ”  â”‚                                            â”‚
â”‚     â”‚                                            â”‚
â”‚ â”€â”€  â”‚         [Ãrea de chat principal]          â”‚
â”‚ ğŸ“„  â”‚                                            â”‚
â”‚ ğŸ“„  â”‚                                            â”‚
â”‚ ğŸ“„  â”‚                                            â”‚
â”‚ ğŸ“„  â”‚                                            â”‚
â”‚     â”‚                                            â”‚
â”‚ â—€â–¶  â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”‚
â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

### ğŸ“Š 3.2 Indicador de Janela de Contexto

Programadores precisam visualizar o consumo de tokens para otimizar prompts e evitar truncamentos.

#### Requisitos Funcionais

- Barra de progresso visual mostrando tokens usados/disponÃ­veis
- Porcentagem e valores absolutos (ex: `3.2k / 4k - 80%`)
- Cores indicativas:
  - ğŸŸ¢ Verde: < 50%
  - ğŸŸ¡ Amarelo: 50-80%
  - ğŸ”´ Vermelho: > 80%
- Alerta quando atingir 90% do limite
- BotÃ£o para "resumir/comprimir conversa"
- Tooltip com breakdown: sistema, histÃ³rico, Ãºltima mensagem

#### LocalizaÃ§Ã£o Sugerida

Posicionar no canto superior direito da Ã¡rea de chat ou como footer fixo abaixo do input.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  [â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‘â–‘â–‘â–‘] 3.2k / 4k tokens (80%)  âš ï¸  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### Limites por Modelo

| Modelo | Limite de Contexto |
|--------|-------------------|
| Llama 3.2 (3B) | 8,192 tokens |
| Llama 3.1 (8B) | 128,000 tokens |
| CodeLlama | 16,384 tokens |
| Mistral | 32,768 tokens |

---

### ğŸ”¤ 3.3 Sistema de TokenizaÃ§Ã£o

Visualizar como o texto Ã© tokenizado ajuda desenvolvedores a entender o comportamento do modelo e otimizar prompts.

#### Requisitos Funcionais

- Contador de tokens em tempo real no campo de input
- Modo de visualizaÃ§Ã£o de tokens (opcional, toggle)
- Highlight de tokens individuais com cores alternadas
- Estimativa de tokens antes de enviar
- HistÃ³rico de tokens por mensagem

#### ImplementaÃ§Ã£o TÃ©cnica

- Usar `tiktoken` (OpenAI) ou tokenizer do Hugging Face
- Cache de tokenizaÃ§Ã£o para performance
- Debounce de 300ms no input para evitar excesso de cÃ¡lculos
- Web Worker para nÃ£o bloquear UI

#### Exemplo Visual

```
Input: "Como fazer um loop em Python?"

Tokens: [Como] [fazer] [um] [loop] [em] [Python] [?]
         1      2       3     4     5      6      7

Total: 7 tokens
```

---

## 4. Melhorias de Frontend

### ğŸ¨ 4.1 Interface e ExperiÃªncia do UsuÃ¡rio

#### Melhorias Essenciais

- Syntax highlighting completo (Prism.js ou Highlight.js)
- BotÃ£o "Copiar cÃ³digo" em cada bloco de cÃ³digo
- Seletor de linguagem nos blocos de cÃ³digo
- Toggle dark/light mode
- Indicador de "digitando..." quando modelo processa
- Streaming de resposta (token por token)
- Markdown rendering completo (tabelas, listas, etc)

#### Melhorias AvanÃ§adas

- Diff viewer para comparar versÃµes de cÃ³digo
- Terminal integrado para testar comandos
- Upload de arquivos para contexto
- Preview de Markdown no input
- MenÃ§Ãµes a arquivos (`@arquivo.py`)
- Snippets salvos/favoritos
- Exportar conversa (MD, PDF, JSON)

---

### âŒ¨ï¸ 4.2 Atalhos de Teclado

| Atalho | AÃ§Ã£o |
|--------|------|
| `Ctrl + Enter` | Enviar mensagem |
| `Ctrl + B` | Toggle sidebar |
| `Ctrl + N` | Nova conversa |
| `Ctrl + K` | Busca rÃ¡pida |
| `Ctrl + /` | Mostrar atalhos |
| `Esc` | Cancelar geraÃ§Ã£o |
| `â†‘` (no input vazio) | Editar Ãºltima mensagem |
| `Ctrl + Shift + C` | Copiar Ãºltimo cÃ³digo |

---

### ğŸ“± 4.3 Responsividade

- Layout adaptativo para tablets (768px - 1024px)
- VersÃ£o mobile com sidebar em overlay
- Touch gestures: swipe para abrir sidebar
- Teclado virtual nÃ£o sobrepondo input

---

## 5. Funcionalidades EspecÃ­ficas para Programadores

### ğŸ’» 5.1 IntegraÃ§Ã£o com CÃ³digo

- DetecÃ§Ã£o automÃ¡tica de linguagem em blocos de cÃ³digo
- FormataÃ§Ã£o automÃ¡tica (Prettier integration)
- Lint warnings inline
- ExecuÃ§Ã£o de cÃ³digo sandboxed (Python, JS)
- IntegraÃ§Ã£o com GitHub Gist para compartilhar

### ğŸ”§ 5.2 Ferramentas de Debug

- Modo verbose para ver raw API calls
- Log de requisiÃ§Ãµes/respostas
- MÃ©tricas de latÃªncia
- Replay de conversas para debug

### ğŸ“ 5.3 Templates e Prompts

- Biblioteca de prompts para tarefas comuns:
  - "Explique este cÃ³digo"
  - "Encontre bugs"
  - "Otimize performance"
  - "Escreva testes"
  - "Documente funÃ§Ã£o"
- Templates customizÃ¡veis
- VariÃ¡veis em prompts (`{linguagem}`, `{framework}`)
- Compartilhamento de prompts

---

## 6. ConfiguraÃ§Ãµes do Sistema

### âš™ï¸ 6.1 ConfiguraÃ§Ãµes de Modelo

- Seletor de modelo (dropdown com modelos disponÃ­veis)
- Ajuste de temperatura (0.0 - 2.0)
- ConfiguraÃ§Ã£o de `max_tokens`
- Top-p / Top-k sampling
- System prompt customizÃ¡vel
- Presets salvos (criativo, preciso, cÃ³digo)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Modelo: [Llama 3.1 8B        â–¼]    â”‚
â”‚                                     â”‚
â”‚ Temperatura:  [â”â”â”â”â”â—â”â”â”â”] 0.7     â”‚
â”‚ Max Tokens:   [â”â”â”â”â”â”â”â—â”â”] 2048    â”‚
â”‚ Top-p:        [â”â”â”â”â”â”â—â”â”â”] 0.9     â”‚
â”‚                                     â”‚
â”‚ [Salvar Preset] [Resetar]          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ‘¤ 6.2 PreferÃªncias do UsuÃ¡rio

- Tema (dark/light/system)
- Tamanho de fonte
- Atalhos customizÃ¡veis
- Idioma da interface
- NotificaÃ§Ãµes

---

## 7. PriorizaÃ§Ã£o de ImplementaÃ§Ã£o

### Fase 1 - MVP (1-2 semanas)

| # | Funcionalidade | Prioridade | EsforÃ§o |
|---|----------------|------------|---------|
| 1 | Corrigir container Ollama | ğŸ”´ CrÃ­tica | 1 dia |
| 2 | Sidebar de histÃ³rico bÃ¡sica | ğŸ”´ Alta | 3 dias |
| 3 | Contador de tokens no input | ğŸŸ¡ MÃ©dia | 1 dia |
| 4 | BotÃ£o copiar cÃ³digo | ğŸŸ¡ MÃ©dia | 0.5 dia |
| 5 | Atalhos de teclado bÃ¡sicos | ğŸŸ¡ MÃ©dia | 1 dia |

### Fase 2 - Essencial (2-4 semanas)

| # | Funcionalidade | Prioridade | EsforÃ§o |
|---|----------------|------------|---------|
| 6 | Indicador de janela de contexto | ğŸŸ¡ MÃ©dia | 2 dias |
| 7 | Streaming de resposta | ğŸ”´ Alta | 2 dias |
| 8 | ConfiguraÃ§Ãµes de modelo | ğŸŸ¡ MÃ©dia | 3 dias |
| 9 | Busca no histÃ³rico | ğŸŸ¢ Baixa | 2 dias |
| 10 | Dark/Light mode toggle | ğŸŸ¢ Baixa | 1 dia |

### Fase 3 - AvanÃ§ado (4-8 semanas)

- VisualizaÃ§Ã£o de tokenizaÃ§Ã£o
- Templates de prompts
- ExecuÃ§Ã£o de cÃ³digo sandboxed
- IntegraÃ§Ã£o com GitHub
- Diff viewer
- Export de conversas

---

## 8. ConsideraÃ§Ãµes TÃ©cnicas

### ğŸ³ 8.1 Problema Atual: Container Ollama

> **Erro:** `dependency failed to start: container focus-ai-ollama-1 is unhealthy`

O Ollama nÃ£o estÃ¡ iniciando corretamente, bloqueando toda a aplicaÃ§Ã£o.

#### DiagnÃ³stico Recomendado

```bash
# Verificar logs do container
docker-compose logs -f ollama

# Ou diretamente
docker logs focus-ai-ollama-1

# Verificar uso de recursos
docker stats focus-ai-ollama-1
```

#### Causas Comuns

1. **RAM insuficiente** - Modelos LLM precisam de muita memÃ³ria
2. **GPU nÃ£o configurada** - Drivers NVIDIA/CUDA ausentes
3. **Timeout curto** - Healthcheck expira antes do modelo carregar
4. **Porta ocupada** - 11434 jÃ¡ em uso

#### SoluÃ§Ãµes

```yaml
# docker-compose.yml - Aumentar timeout do healthcheck
ollama:
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
    interval: 30s
    timeout: 120s  # Aumentar de 30s para 120s
    retries: 5
    start_period: 60s  # Dar tempo para iniciar
```

```bash
# PrÃ©-baixar modelo menor para teste
docker exec -it focus-ai-ollama-1 ollama pull tinyllama
```

---

### ğŸ“¦ 8.2 Stack TecnolÃ³gica Recomendada

| Componente | Tecnologia |
|------------|------------|
| Frontend | React + TypeScript + Vite |
| EstilizaÃ§Ã£o | Tailwind CSS + shadcn/ui |
| Estado | Zustand ou Jotai |
| Syntax Highlight | Prism.js ou Shiki |
| Markdown | react-markdown + remark-gfm |
| TokenizaÃ§Ã£o | tiktoken (via WASM) |
| Backend | FastAPI (jÃ¡ existente) |
| LLM | Ollama (jÃ¡ existente) |
| Database | PostgreSQL (jÃ¡ existente) |
| Cache | Redis (jÃ¡ existente) |

---

### ğŸ”Œ 8.3 Estrutura de Componentes Sugerida

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ Chat/
â”‚   â”‚   â”œâ”€â”€ ChatWindow.tsx
â”‚   â”‚   â”œâ”€â”€ MessageList.tsx
â”‚   â”‚   â”œâ”€â”€ MessageBubble.tsx
â”‚   â”‚   â”œâ”€â”€ CodeBlock.tsx
â”‚   â”‚   â””â”€â”€ InputArea.tsx
â”‚   â”œâ”€â”€ Sidebar/
â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”œâ”€â”€ ConversationList.tsx
â”‚   â”‚   â”œâ”€â”€ ConversationItem.tsx
â”‚   â”‚   â””â”€â”€ SearchBar.tsx
â”‚   â”œâ”€â”€ Context/
â”‚   â”‚   â”œâ”€â”€ TokenCounter.tsx
â”‚   â”‚   â””â”€â”€ ContextIndicator.tsx
â”‚   â””â”€â”€ Settings/
â”‚       â”œâ”€â”€ SettingsModal.tsx
â”‚       â”œâ”€â”€ ModelConfig.tsx
â”‚       â””â”€â”€ Preferences.tsx
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useChat.ts
â”‚   â”œâ”€â”€ useTokenizer.ts
â”‚   â””â”€â”€ useConversations.ts
â”œâ”€â”€ stores/
â”‚   â”œâ”€â”€ chatStore.ts
â”‚   â””â”€â”€ settingsStore.ts
â””â”€â”€ utils/
    â”œâ”€â”€ tokenizer.ts
    â””â”€â”€ api.ts
```

---

## 9. ConclusÃ£o

O Focus AI tem uma base sÃ³lida com FastAPI, Ollama e infraestrutura Docker. Para se tornar uma ferramenta competitiva para programadores, precisa priorizar:

1. **Resolver o problema do container Ollama** (blocker)
2. **Implementar sidebar de histÃ³rico** para UX bÃ¡sica
3. **Adicionar indicadores de contexto/tokens**
4. **Melhorar experiÃªncia de cÃ³digo** (copiar, highlight, streaming)

Com essas melhorias implementadas nas fases sugeridas, o Focus AI estarÃ¡ bem posicionado como uma ferramenta de chat LLM especializada para desenvolvedores.

---

*Documento gerado em Janeiro 2026*
