# Debug Mode - Implementa√ß√£o T√©cnica

## Arquitetura

### Fluxo de Dados

```
Frontend (Chat.jsx)
    ‚Üì debug_mode: true
Backend (session_routes.py)
    ‚Üì QuestionRequest { content, debug_mode }
Use Case (ask_question.py)
    ‚Üì execute(..., debug_mode)
Chain Validator (chain_validator_service.py)
    ‚Üì generate_answer(..., debug_mode)
Senior LLM (senior_llm_service.py)
    ‚Üì generate_debug(question)
Gemini 2.5 Pro (Debug Model)
    ‚Üì Resposta estruturada
Frontend (Renderiza√ß√£o)
```

## Componentes Modificados

### 1. Frontend (`Chat.jsx`)

**Estado:**
```javascript
const [debugMode, setDebugMode] = useState(false);
```

**Request:**
```javascript
body: JSON.stringify({ 
  content: currentInput,
  debug_mode: debugMode 
})
```

**UI Indicators:**
- Badge "DEBUG" no header
- Bot√£o com anima√ß√£o pulse
- Banner informativo
- Input com borda vermelha
- Placeholder customizado

### 2. Backend Routes (`session_routes.py`)

**Request Model:**
```python
class QuestionRequest(BaseModel):
    content: str
    debug_mode: bool = False
```

**Endpoint:**
```python
result = use_case.execute(
    session_id, 
    user_id, 
    request.content, 
    request.debug_mode
)
```

### 3. Use Case (`ask_question.py`)

**Signature:**
```python
def execute(
    self, 
    session_id: str, 
    user_id: str, 
    content: str, 
    debug_mode: bool = False
) -> dict:
```

**LLM Call:**
```python
llm_response = self.llm_service.generate_answer(
    content, 
    debug_mode=debug_mode
)
```

### 4. Chain Validator (`chain_validator_service.py`)

**L√≥gica Principal:**
```python
def generate_answer(
    self, 
    question: str, 
    conversation_history: list = None, 
    debug_mode: bool = False
) -> dict:
    
    # Debug Mode: Usa Senior diretamente
    if debug_mode:
        senior_result = self.senior.generate_debug(
            question, 
            conversation_history
        )
        return {
            "content": senior_result["content"],
            "model": "gemini-2.5-pro-debug",
            "used_senior": True
        }
    
    # Modo normal: Junior ‚Üí Senior (se necess√°rio)
    # ...
```

### 5. Senior LLM (`senior_llm_service.py`)

**Debug Model:**
```python
self.debug_model = genai.GenerativeModel(
    model_name='gemini-2.5-pro',
    generation_config={"temperature": 0.2},
    system_instruction="""
    üéØ MODO DEBUG ATIVADO - An√°lise T√©cnica Profunda
    
    Para CADA pergunta, forne√ßa:
    1. üîç AN√ÅLISE DETALHADA
    2. üéØ CAUSAS RAIZ
    3. üí° SOLU√á√ïES PR√ÅTICAS
    4. ‚úÖ MELHORES PR√ÅTICAS
    5. üèóÔ∏è ARQUITETURA & ESCALABILIDADE
    """
)
```

**M√©todo Debug:**
```python
def generate_debug(
    self, 
    question: str, 
    conversation_history: list = None
) -> dict:
    debug_prompt = f"""
    üêõ DEBUG MODE - An√°lise T√©cnica Profunda
    
    Pergunta: {question}
    
    Forne√ßa uma an√°lise COMPLETA seguindo a estrutura:
    1. üîç AN√ÅLISE DETALHADA
    2. üéØ CAUSAS RAIZ  
    3. üí° SOLU√á√ïES PR√ÅTICAS (com c√≥digo)
    4. ‚úÖ MELHORES PR√ÅTICAS
    5. üèóÔ∏è ARQUITETURA & ESCALABILIDADE
    """
    
    response = self.debug_model.generate_content(debug_prompt)
    return {"content": response.text, "debug_mode": True}
```

## Prompt Engineering

### System Instruction (Debug Model)

O prompt do Debug Model √© otimizado para:

1. **Profundidade T√©cnica**: Temperature 0.2 (mais determin√≠stico)
2. **Estrutura Consistente**: 5 se√ß√µes obrigat√≥rias
3. **Exemplos Pr√°ticos**: C√≥digo funcional e testado
4. **M√∫ltiplas Linguagens**: Python, JS, Go, Rust, Java, C++
5. **Contexto de Produ√ß√£o**: Escalabilidade e seguran√ßa

### Estrutura da Resposta

```markdown
# üîç AN√ÅLISE DETALHADA
- Problema identificado
- Contexto t√©cnico
- O que acontece internamente

# üéØ CAUSAS RAIZ
- Lista de poss√≠veis causas
- Causa mais prov√°vel
- Explica√ß√£o do "por qu√™"

# üí° SOLU√á√ïES PR√ÅTICAS
## Solu√ß√£o 1: [Nome]
```language
// C√≥digo completo
```
Trade-offs: ...

## Solu√ß√£o 2: [Nome]
```language
// C√≥digo alternativo
```
Trade-offs: ...

# ‚úÖ MELHORES PR√ÅTICAS
- Padr√µes da ind√∫stria
- Otimiza√ß√µes
- Seguran√ßa

# üèóÔ∏è ARQUITETURA & ESCALABILIDADE
- Como escalar
- Patterns recomendados
- Considera√ß√µes de produ√ß√£o
```

## Performance

### Compara√ß√£o de Custos

| Modo | Modelo | Custo/Request | Lat√™ncia |
|------|--------|---------------|----------|
| Normal (Junior) | Gemini 2.0 Flash Lite | ~$0.001 | ~500ms |
| Normal (Senior) | Gemini 2.5 Pro | ~$0.003 | ~1500ms |
| Debug Mode | Gemini 2.5 Pro | ~$0.003 | ~2000ms |

### Otimiza√ß√µes

1. **Cache**: Respostas s√£o cacheadas no Redis
2. **Skip Junior**: Debug Mode pula valida√ß√£o desnecess√°ria
3. **Temperature**: 0.2 para respostas mais consistentes
4. **Streaming**: Futuro - streaming de respostas longas

## Seguran√ßa

### Valida√ß√µes

1. **Autentica√ß√£o**: JWT token obrigat√≥rio
2. **Rate Limiting**: Redis locks por sess√£o
3. **Input Sanitization**: Valida√ß√£o de conte√∫do
4. **Output Filtering**: Sem dados sens√≠veis

### Logs

```python
logger.info(f"Processing question: {question[:50]}... [DEBUG={debug_mode}]")
logger.info("Debug Mode activated - using Senior directly")
logger.info("Debug mode generation completed")
```

## Testes

### Unit√°rios

- `test_debug_mode_uses_senior_directly`
- `test_normal_mode_uses_junior_first`
- `test_debug_mode_prompt_structure`
- `test_debug_mode_fallback_on_error`

### Integra√ß√£o

- `test_debug_mode_end_to_end`

### Executar Testes

```bash
cd backend
pytest tests/test_debug_mode.py -v
```

## Monitoramento

### M√©tricas

1. **Taxa de Uso**: % de requests com debug_mode=true
2. **Custo**: Gasto total com Senior LLM
3. **Lat√™ncia**: Tempo m√©dio de resposta
4. **Satisfa√ß√£o**: Feedback dos usu√°rios

### Logs Estruturados

```json
{
  "timestamp": "2024-01-15T10:30:00Z",
  "level": "INFO",
  "message": "Debug mode activated",
  "session_id": "abc123",
  "user_id": "user456",
  "model": "gemini-2.5-pro-debug",
  "latency_ms": 2150
}
```

## Roadmap

### V1 (Atual) ‚úÖ
- [x] Toggle Debug Mode no frontend
- [x] Backend processa flag debug_mode
- [x] Senior LLM com prompt especializado
- [x] UI indicators (badge, tooltip, banner)
- [x] Testes unit√°rios

### V2 (Pr√≥ximo)
- [ ] Streaming de respostas longas
- [ ] Cache inteligente por tipo de pergunta
- [ ] M√©tricas de uso no dashboard
- [ ] Export de an√°lises em Markdown
- [ ] Templates de debug pr√©-configurados

### V3 (Futuro)
- [ ] Debug Mode com hist√≥rico de sess√£o
- [ ] Integra√ß√£o com VS Code
- [ ] An√°lise de c√≥digo em tempo real
- [ ] Sugest√µes proativas de otimiza√ß√£o
- [ ] Compara√ß√£o de solu√ß√µes lado a lado

## Troubleshooting

### Problema: Debug Mode n√£o ativa

**Causa**: Estado n√£o sincronizado
**Solu√ß√£o**: Verificar console do navegador

### Problema: Resposta igual ao modo normal

**Causa**: Flag n√£o chegou ao backend
**Solu√ß√£o**: Verificar network tab, payload deve ter `debug_mode: true`

### Problema: Erro 500 no backend

**Causa**: Gemini API error
**Solu√ß√£o**: Verificar logs, fallback para modelo normal

### Problema: Resposta muito longa

**Causa**: Debug Mode √© verbose por design
**Solu√ß√£o**: Implementar streaming (V2)

## Contribuindo

### Adicionar Nova Se√ß√£o ao Debug

1. Editar `senior_llm_service.py`
2. Atualizar `system_instruction` do `debug_model`
3. Adicionar emoji e estrutura
4. Testar com perguntas variadas
5. Atualizar documenta√ß√£o

### Melhorar Prompt

1. Testar com casos reais
2. Iterar no `system_instruction`
3. Ajustar `temperature` se necess√°rio
4. Validar com testes A/B
5. Documentar mudan√ßas

---

**Desenvolvido com ‚ù§Ô∏è pela equipe Focus AI**
